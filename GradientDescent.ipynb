{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1-final"
    },
    "colab": {
      "name": "GradientDescent.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zc-oO/python-interview-prep/blob/master/GradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUXUbvIcbmSy"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPlLJzl9bmS6",
        "outputId": "bba12bc6-3a4f-4d32-d7ec-997f7ba13103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dim_theta = 10\n",
        "data_num = 1000\n",
        "scale = .1\n",
        "\n",
        "theta_true = np.ones((dim_theta,1))\n",
        "print('True theta:', theta_true.reshape(-1))\n",
        "\n",
        "A = np.random.uniform(low=-1.0, high=1.0, size=(data_num,dim_theta))\n",
        "y_data = A @ theta_true + np.random.normal(loc=0.0, scale=scale, size=(data_num, 1))\n",
        "\n",
        "A_test = np.random.uniform(low=-1.0, high=1.0, size=(50, dim_theta))\n",
        "y_test = A_test @ theta_true + np.random.normal(loc=0.0, scale=scale, size=(50, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True theta: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU_XvbFtbmTT"
      },
      "source": [
        "# Solving for the exact mean squared loss (solving Ax = b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmhrX7mHbmTW",
        "outputId": "90bda27c-733c-411d-bcc9-84e5d2b4b0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print('Not implemented.')\n",
        "\n",
        "'''\n",
        "Hints:\n",
        "1. See the least squares solution to Ax = b (when it is covered in lecture).\n",
        "\n",
        "2. Use Numpy functions like Numpy's linear algebra functions to solve for x in Ax = b.\n",
        "In fact, the linear algebra module is already imported with ```import numpy.linalg as la```.\n",
        "\n",
        "3. Use the defined variable A in Ax = b. Use y_data as b. Use theta_pred as x.\n",
        "'''\n",
        "theta_pred = np.dot(np.linalg.inv(np.dot(A.T, A)), np.dot(A.T, y_data)) # TODO: Implement the analytical solution\n",
        "\n",
        "print('Empirical theta', theta_pred.reshape(-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empirical theta [0.99381457 1.0007508  1.0069903  0.98793512 0.99774418 1.00060224\n",
            " 0.99775653 0.99881946 0.99399647 1.00632527]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeV_SKF_bmTj"
      },
      "source": [
        "# SGD Variants Noisy Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvwmLrlAbmTn"
      },
      "source": [
        "batch_size = 1\n",
        "max_iter = 1000\n",
        "lr = 0.001\n",
        "theta_init = np.random.random((10,1)) * 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVSuTZxbmT2"
      },
      "source": [
        "def noisy_val_grad(theta_hat, data_, label_, deg_=2.):\n",
        "    gradient = np.zeros_like(theta_hat)\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(data_.shape[0]):\n",
        "        x_ = data_[i, :].reshape(-1,1)\n",
        "        y_ = label_[i, 0]\n",
        "        err = np.sum(x_ * theta_hat) - y_\n",
        "\n",
        "        # print('Not implemented.')\n",
        "\n",
        "        '''\n",
        "        Hints:\n",
        "        1. Find the gradient and loss for each data point x_.\n",
        "        2. For grad, you need err, deg_, and x_.\n",
        "        3. For l, you need err and deg_ only.\n",
        "        4. Checkout the writeup for more hints.\n",
        "        '''\n",
        "        grad = deg_ * np.abs(err) ** (deg_ - 1) * np.sign(err) * x_ # TODO: Implement the analytical gradient\n",
        "        l = np.abs(err) ** deg_ # TODO: Implement the loss function\n",
        "\n",
        "        loss += l / data_.shape[0]\n",
        "        gradient += grad / data_.shape[0]\n",
        "\n",
        "    return loss, gradient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5abUl0ibmUF"
      },
      "source": [
        "# Running SGD Variants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xybXkEd7bmUJ",
        "outputId": "627ff8d7-1337-46ed-a521-e81812b9d1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "#@title Parameters\n",
        "deg_ = 2. #@param {type: \"number\"}\n",
        "num_rep = 10 #@param {type: \"integer\"}\n",
        "max_iter = 1000 #@param {type: \"integer\"}\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "best_vals = {}\n",
        "test_exp_interval = 50 #@param {type: \"integer\"}\n",
        "grad_artificial_normal_noise_scale = 0. #@param {type: \"number\"}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM80QBNFbmUV"
      },
      "source": [
        "for method_idx, method in enumerate(['adam', 'sgd', 'adagrad']):\n",
        "    test_loss_mat = []\n",
        "    train_loss_mat = []\n",
        "\n",
        "    for replicate in range(num_rep):\n",
        "        if replicate % 20 == 0:\n",
        "            print(method, replicate)\n",
        "\n",
        "        if method == 'adam':\n",
        "            # print('Adam Not implemented.')\n",
        "            beta_1 = 0.9\n",
        "            beta_2 = 0.999\n",
        "            m = np.zeros((len(A[0]), 1)) # TODO: Initialize parameters\n",
        "            v = np.zeros((len(A[0]), 1))\n",
        "            epsilon = 10**-8\n",
        "\n",
        "        if method == 'adagrad':\n",
        "            # print('Adagrad Not implemented.')\n",
        "            epsilon = 10**-8 # TODO: Initialize parameters\n",
        "            squared_sum = np.zeros((len(A[0]), 1))\n",
        "\n",
        "        theta_hat = theta_init.copy()\n",
        "        test_loss_list = []\n",
        "        train_loss_list = []\n",
        "\n",
        "        for t in range(max_iter):\n",
        "            idx = np.random.choice(data_num, batch_size) # Split data\n",
        "            train_loss, gradient = noisy_val_grad(theta_hat, A[idx,:], y_data[idx,:], deg_=deg_)\n",
        "            artificial_grad_noise = np.random.randn(10, 1) * grad_artificial_normal_noise_scale + np.sign(np.random.random((10, 1)) - 0.5) * 0.\n",
        "            gradient = gradient + artificial_grad_noise\n",
        "            train_loss_list.append(train_loss)\n",
        "\n",
        "            if t % test_exp_interval == 0:\n",
        "                test_loss, _ = noisy_val_grad(theta_hat, A_test[:,:], y_test[:,:], deg_=deg_)\n",
        "                test_loss_list.append(test_loss)\n",
        "\n",
        "            if method == 'adam':\n",
        "                # print('Adam Not implemented.') # TODO: Implement Adam\n",
        "                m = beta_1 * m + (1 - beta_1) * gradient\n",
        "                v = beta_2 * v + (1 - beta_2) * (gradient ** 2)\n",
        "                m_hat = m / (1 - beta_1 ** (t + 1))\n",
        "                v_hat = v / (1 - beta_2 ** (t + 1))\n",
        "                theta_hat = theta_hat - lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "\n",
        "            elif method == 'adagrad':\n",
        "                # print('Adagrad Not implemented.')\n",
        "                squared_sum = squared_sum + gradient ** 2 # TODO: Implement Adagrad\n",
        "                theta_hat = theta_hat - lr * gradient / (np.sqrt(squared_sum) + epsilon)\n",
        "\n",
        "            elif method == 'sgd':\n",
        "                theta_hat = theta_hat - lr * gradient\n",
        "\n",
        "        test_loss_mat.append(test_loss_list)\n",
        "        train_loss_mat.append(train_loss_list)\n",
        "\n",
        "    print(method, 'done')\n",
        "    x_axis = np.arange(max_iter)[::test_exp_interval]\n",
        "\n",
        "    print('test_loss_np is a 2d array with num_rep rows and each column denotes a specific update stage in training')\n",
        "    print('The elements of test_loss_np are the test loss values computed in each replicate and training stage.')\n",
        "    test_loss_np = np.array(test_loss_mat)\n",
        "\n",
        "    print('Not implemented.')\n",
        "    '''\n",
        "    Hints:\n",
        "    1. Use test_loss_np in np.mean() with axis = 0\n",
        "    '''\n",
        "    test_loss_mean = np.mean(test_loss_np, axis=0) # TODO: Calculate the mean test loss\n",
        "\n",
        "    '''\n",
        "    Hints:\n",
        "    1. Use test_loss_np in np.std() with axis = 0\n",
        "    2. Divide by np.sqrt() using num_rep as a parameter\n",
        "    '''\n",
        "    test_loss_se = np.std(test_loss_np, axis=0) / np.sqrt(num_rep) # TODO: Calculate the standard error for test loss\n",
        "\n",
        "    plt.errorbar(x_axis, test_loss_mean, yerr=2.5*test_loss_se, label=method)\n",
        "    best_vals[method] = min(test_loss_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGmQD7nDbmUg"
      },
      "source": [
        "best_vals = { k: int(v * 1000) / 1000. for k,v in best_vals.items() } # A weird way to round numbers\n",
        "plt.title(f'Test Loss \\n(objective degree: {deg_},  best values: {best_vals})')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Updates')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}